#### LAYER 1 CONFIGURATION

# we define the node_bridge_config vars here and not in the host-specific config file,
# because it depends on the rollout scenario (all-in-one vs. multi-node) which interfaces
# are being bridged and whether or not additional IPs need to be configured (e.g. for admin IP to the ndoes)
[sddc]
storm2.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em2'}, 'admin':{'nic':'p5p1', 'host_prefix':'10.116.127.242/24'} 'services':{'nic':'p5p1'}, 'internal_api':{'nic':'p5p1'}, 'tenant':{'nic':'p5p1'}, 'storage':{'nic':'p5p1'}, 'storage_mgmt':{'nic':'p5p1'}, 'rhosp_provisioning':{'nic':'p5p3'}, 'provider1':{'nic':'p5p1'}, 'provider2':{'nic':'p5p1'} }"
storm3.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em2'}, 'admin':{'nic':'p5p1', 'host_prefix':'10.116.127.243/24'} 'services':{'nic':'p5p1'}, 'internal_api':{'nic':'p5p1'}, 'tenant':{'nic':'p5p1'}, 'storage':{'nic':'p5p1'}, 'storage_mgmt':{'nic':'p5p1'}, 'rhosp_provisioning':{'nic':'p5p3'}, 'provider1':{'nic':'p5p1'}, 'provider2':{'nic':'p5p1'} }"
storm4.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.4/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.244/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'rhosp_provisioning':{'nic':'em3'}, 'provider1':{'nic':'p2p1'}, 'provider2':{'nic':'p2p1'} }"
storm5.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.5/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.245/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'rhosp_provisioning':{'nic':'em3'}, 'provider1':{'nic':'p2p1'}, 'provider2':{'nic':'p2p1'} }"
storm6.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.6/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}{% if infrastructure_network_master is defined %}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.246/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'rhosp_provisioning':{'nic':'em3'}, 'provider1':{'nic':'p2p1'}, 'provider2':{'nic':'p2p1'}{% endif %} }"

[sddc:vars]
ansible_ssh_common_args="-i ./binary/{{ hailstorm_ssh_priv_key_file }}"

# the layer1 group should only contain a single server called layer1
[layer1]
layer1          ansible_host=10.116.127.1 node_bridge_config='{{ hostvars[layer1_ansible_host].node_bridge_config }}'

#### VM CONFIGURATION (LAYER 2 AND LAYER 3)

[satellite]
satellite       ansible_host=10.116.127.10

# the IPA server
[ipa]
ipa             ansible_host=10.116.127.11 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

# the single node that is RHEV-Manager
[rhevm]
rhevm           ansible_host=10.116.127.12 activation_key="AK-CV-RHEV-MANAGER-{{ stage }}" additional_dns="rhev"

# the nodes that are RHEV-Hypervisor
[rhevh]
rhevh1          ansible_host=10.116.127.13 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"
rhevh2          ansible_host=10.116.127.14 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"
rhevh3          ansible_host=10.116.127.15 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"

# the CloudForms management engine
[cloudforms]
cloudforms      ansible_host=10.116.127.17 activation_key="AK-CV-CLOUDFORMS-{{ stage }}"

[test-rhel6]
test-rhel6      ansible_host=10.116.127.18 activation_key="AK-CV-OS-RHEL6-SERVER-{{ stage }}"

[test-rhel7]
test-rhel7      ansible_host=10.116.127.19 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[ose3-master]
ose3-master1    ansible_host=10.116.127.20 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"
ose3-master2    ansible_host=10.116.127.21 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"
ose3-master3    ansible_host=10.116.127.22 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"

[ose3-node]
ose3-infranode1 ansible_host=10.116.127.23 openshift_node_labels="{'region': 'infra', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}" additional_dns="*.apps,apps"
ose3-node1      ansible_host=10.116.127.25 openshift_node_labels="{'region': 'primary', 'zone': 'east', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}"
ose3-node2      ansible_host=10.116.127.26 openshift_node_labels="{'region': 'primary', 'zone': 'west', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}"

[ose3-lb]
ose3-lb         ansible_host=10.116.127.27 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="openshift"

[ose3-osp-master]
ose3-osp-master1    openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-master2    openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-master3    openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"

[ose3-osp-node]
ose3-osp-infranode1 openshift_node_labels="{'region': 'infra', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}" additional_dns="*.apps-osp,apps-osp" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-node1      openshift_node_labels="{'region': 'primary', 'zone': 'east', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-node2      openshift_node_labels="{'region': 'primary', 'zone': 'west', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.large" osp_image="RHEL7"

[ose3-osp-lb]
ose3-osp-lb         activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="openshift-osp" osp_tenant="demo-infra" osp_flavor="m1.medium" osp_image="RHEL7"

# Elasticsearch, FluentD, Kibana
[efk]
efk             ansible_host=10.116.127.28 activation_key="AK-CV-RHOSP-{{ stage }}"

# the Infrastructure server (DNS, SMTP/IMAP)
[infrastructure]
infrastructure  ansible_host=10.116.127.29 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="imap,smtp"

# The single system where osp-director will be installed to
[rhosp-director]
rhosp-director  ansible_host=10.116.127.30 activation_key="AK-CV-RHOSP-{{ stage }}"

# The group of all osp compute nodes
[rhosp-compute]
rhosp-compute1  ansible_host=10.116.127.31 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute2  ansible_host=10.116.127.32 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute3  ansible_host=10.116.127.33 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute4  ansible_host=10.116.127.40 activation_key="AK-CV-RHOSP-{{ stage }}"

# The group of all osp control nodes
[rhosp-control]
rhosp-control1  ansible_host=10.116.127.34 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-control2  ansible_host=10.116.127.35 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-control3  ansible_host=10.116.127.36 activation_key="AK-CV-RHOSP-{{ stage }}"

[dev-client]
dev-client     ansible_host=10.116.127.37 activation_key="AK-CV-RHOSE3-CLIENT-{{ stage }}"

[tower]
tower          ansible_host=10.116.127.38 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[lookbusy-rhev]
lookbusy-rhev   ansible_host=10.116.127.39 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" rhev_image="RHEL7" nic_attachments="[ {{ infrastructure_network_services }},{{ infrastructure_network_admin }} ]"

[ceph]
#will be filled in dynamically via layer2_ceph_inventory role
#ceph1 ansible_host=10.116.127.2 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph2 ansible_host=10.116.127.3 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph3 ansible_host=10.116.127.4 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph4 ansible_host=10.116.127.5 activation_key="AK-CV-CEPH-{{ stage }}"

[storage-console]
storage-console ansible_host=10.116.127.9 activation_key="AK-CV-CEPH-{{ stage }}"

[install-host]
install-host ansible_host=10.116.127.6 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[rh-sso]
rh-sso ansible_host=10.116.127.7 activation_key="AK-CV-RH-SSO-{{ stage }}"

[proxy-group]
proxy ansible_host=10.116.127.50 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"
# not very happy about .50 since it is in the IP range used by undercloud provisioning, but since the VM doesn't share the network...

[nuage]
vsd1 ansible_host=10.116.120.101 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
vsd2 ansible_host=10.116.120.102 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
vsd3 ansible_host=10.116.120.103 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
vsc1 ansible_host=10.116.120.104 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}"
vsc2 ansible_host=10.116.120.105 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}"
vsc3 ansible_host=10.116.120.106 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}"
vsc4 ansible_host=10.116.120.107 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}"
elastic1 ansible_host=10.116.120.108 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
elastic2 ansible_host=10.116.120.109 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
elastic3 ansible_host=10.116.120.110 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
vsr1 ansible_host=10.116.120.111 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
vsr2 ansible_host=10.116.120.112 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"

# virtual IPs - these will not be instantiated through Ansible (but need them for DNS records, DNAT, etc)
[vips]
openstack       ansible_host=10.116.120.81 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
undercloud-public ansible_host=10.116.124.40 nic_attachments="[ {{ infrastructure_network_rhosp_provisioning }} ]" default_route_via={{infrastructure_network_rhosp_provisioning}} additional_dns="director"
undercloud-admin  ansible_host=10.116.124.41 nic_attachments="[ {{ infrastructure_network_rhosp_provisioning }} ]"

# layer 3 openstack
[lookbusy-osp]
lookbusy-osp    activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"

[cloudforms-osp-group]
cloudforms-osp ansible_host=10.116.127.121 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.xlarge" osp_image="Cloudforms"
cloudforms-osp-2 ansible_host=10.116.127.122 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.xlarge" osp_image="Cloudforms"

[ansible-jump-host]
ansible-jump-demo-vms  activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-vms" osp_flavor="m1.small" osp_image="RHEL7"
#ansible-jump-demo-infra  activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"

[laptop]
localhost              ansible_connection=local openshift_master_api_host_and_port='openshift.{{ hailstorm_dns_domain }}:443'


#### ADDITIONAL GROUPS

# which system will host virt-who -> this should not be satellite since virt-who reports to subscription-manager which reports to RHN (which doesn't know the Default Organization) on satellite
[virt-who]
infrastructure

# some test nodes to test subscription with satellite
[test-rhel:children]
test-rhel6
test-rhel7

# the list of groups that make up RHEV
[rhev:children]
rhevm
rhevh

# all RHEV nodes and the layer1 host to ensure there's a common view on the storage domain
[layer1-rhev:children]
rhev
layer1

# A list of all OpenStack subgroups
[rhosp-all:children]
rhosp-director
rhosp-undercloud

# The groups that make up the osp undercloud (i.e. compute, control and potentially storage)
[rhosp-undercloud:children]
rhosp-compute
rhosp-control

# a group required to store a common configuration for IPMI emulation between the layer1 host and the osp-director
[layer1-rhosp]
rhosp-director
layer1

# all openshift nodes on RHV
[ose3:children]
ose3-master
ose3-node
ose3-lb

# all openshift nodes on OSP
[ose3-osp:children]
ose3-osp-master
ose3-osp-node
ose3-osp-lb

# common config items for openshift masters and nodes
[ose3-common:children]
ose3-master
ose3-node

# common config items for openshift masters and nodes on OSP
[ose3-osp-common:children]
ose3-osp-master
ose3-osp-node

# the node where the ansible playbook will run
[ose3-installer]
ose3-master1

# the node where the ansible playbook will run on OSP
[ose3-osp-installer]
ose3-osp-master1

# All layer2 node groups that are installed with RHEL7
[rhel7:children]
proxy-group
install-host
rh-sso
nuage
rhosp-all
satellite
rhevh
ose3
ose3-osp
test-rhel7
ipa
infrastructure
efk
lookbusy
dev-client
tower
cloudforms
cloudforms-osp-group
ansible-jump-host
storage-console

# All layer2 node groups that are installed with RHEL6
[rhel6:children]
#rhevm #group assignment (rhel6 or rhel7) now done via role
test-rhel6

# everything on layer2
[layer2:children]
proxy-group
install-host
rh-sso
nuage
ose3
satellite
rhev
rhosp-all
test-rhel6
test-rhel7
ipa
infrastructure
efk
dev-client
tower
storage-console

# to let it participate in the calculation of ansible_host
# and the ssh proxy command used to communicate with all layer2/3 hosts
[accessible_via_admin_network:children]
layer2
cloudforms
cloudforms-osp-group
vips
lookbusy-rhev

[accessible_via_floating_ip:children]
ansible-jump-host
lookbusy-osp

[accessible_via_services_network:children]
nuage

# to let it participate in the calculation of vm_nics
[automated_vmnics_calculation:children]
accessible_via_floating_ip
accessible_via_admin_network
accessible_via_services_network

# A group to capture all nodes with standard 3-NIC network layout (services, admin, storage)
[niclayout-standard:children]
install-host
rh-sso
nuage
satellite
test-rhel
infrastructure
ipa
ose3
efk
rhevm
lookbusy-rhev
dev-client
tower
storage-console

[lookbusy:children]
lookbusy-osp
lookbusy-rhev
