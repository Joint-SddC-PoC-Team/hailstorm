#### LAYER 1 CONFIGURATION

# we define the node_bridge_config vars here and not in the host-specific config file,
# because it depends on the rollout scenario (all-in-one vs. multi-node) which interfaces
# are being bridged and whether or not additional IPs need to be configured (e.g. for admin IP to the ndoes)
[sddc]
storm2.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em2'}, 'admin':{'nic':'p5p1', 'host_prefix':'10.116.127.242/24'} 'services':{'nic':'p5p1'}, 'internal_api':{'nic':'p5p1'}, 'tenant':{'nic':'p5p1'}, 'storage':{'nic':'p5p1'}, 'storage_mgmt':{'nic':'p5p1'}, 'provisioning':{'nic':'p5p3'}, 'bgp1':{'nic':'p5p1'}, 'bgp2':{'nic':'p5p1'}, 'ospf1':{'nic':'p5p1'}, 'ospf2':{'nic':'p5p1'}, 'baremetal':{'nic':'p5p1'} }"
storm3.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em2'}, 'admin':{'nic':'p5p1', 'host_prefix':'10.116.127.243/24'} 'services':{'nic':'p5p1'}, 'internal_api':{'nic':'p5p1'}, 'tenant':{'nic':'p5p1'}, 'storage':{'nic':'p5p1'}, 'storage_mgmt':{'nic':'p5p1'}, 'provisioning':{'nic':'p5p3'}, 'bgp1':{'nic':'p5p1'}, 'bgp2':{'nic':'p5p1'}, 'ospf1':{'nic':'p5p1'}, 'ospf2':{'nic':'p5p1'}, 'baremetal':{'nic':'p5p1'} }"
storm4.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.4/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}{% if infrastructure_network_master is defined %}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.244/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'provisioning':{'nic':'em3'}, 'bgp1':{'nic':'p2p1'}, 'bgp2':{'nic':'p2p1'}, 'ospf1':{'nic':'p2p1'}, 'ospf2':{'nic':'p2p1'}, 'baremetal':{'nic':'p2p1'}{% endif %} }"
storm5.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.5/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}{% if infrastructure_network_master is defined %}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.245/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'provisioning':{'nic':'em3'}, 'bgp1':{'nic':'p2p1'}, 'bgp2':{'nic':'p2p1'}, 'ospf1':{'nic':'p2p1'}, 'ospf2':{'nic':'p2p1'}, 'baremetal':{'nic':'p2p1'}{% endif %} }"
storm6.coe.muc.redhat.com node_bridge_config="{ 'guests':{'nic':'em1','host_prefix':'10.32.105.6/20','default_gw':'10.32.111.254', 'additional':'DEFROUTE=\"yes\"\nDNS1=10.32.96.1\nDNS2=10.32.96.30\nDOMAIN=coe.muc.redhat.com'}{% if infrastructure_network_master is defined %}, 'admin':{'nic':'p2p1','host_prefix':'10.116.127.246/24'}, 'services':{'nic':'p2p1'}, 'internal_api':{'nic':'p2p1'}, 'tenant':{'nic':'p2p1'}, 'storage':{'nic':'p2p1'}, 'storage_mgmt':{'nic':'p2p1'}, 'provisioning':{'nic':'em3'}, 'bgp1':{'nic':'p2p1'}, 'bgp2':{'nic':'p2p1'}, 'ospf1':{'nic':'p2p1'}, 'ospf2':{'nic':'p2p1'}, 'baremetal':{'nic':'p2p1'}{% endif %} }"

[sddc:vars]
ansible_ssh_common_args="-i ./binary/{{ hailstorm_ssh_priv_key_file }}"

# the layer1 group should only contain a single server called layer1
[layer1]
layer1          ansible_host=10.116.127.1 node_bridge_config='{{ hostvars[layer1_ansible_host].node_bridge_config }}'

#### VM CONFIGURATION (LAYER 2 AND LAYER 3)

[satellite]
satellite       ansible_host=10.116.127.152

# the IPA server
[ipa]
ipa             ansible_host=10.116.127.153 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

# the single node that is RHEV-Manager
[rhevm]
#rhevm           ansible_host=10.116.127.12 activation_key="AK-CV-RHEV-MANAGER-{{ stage }}" additional_dns="rhev"

# the nodes that are RHEV-Hypervisor
[rhevh]
#rhevh1          ansible_host=10.116.127.13 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"
#rhevh2          ansible_host=10.116.127.14 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"
#rhevh3          ansible_host=10.116.127.15 activation_key="AK-CV-RHEV-HYPERVISOR-{{ stage }}"

# the CloudForms management engine
#[cloudforms]
#cloudforms      ansible_host=10.116.127.17 activation_key="AK-CV-CLOUDFORMS-{{ stage }}"

#zone 919000000000001
[cloudforms-db]
cloudforms-db01        ansible_host=10.116.127.201 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="db" db=true extra_disks=true

#zone 919000000000002
[cloudforms-ui]
cloudforms-ui01   ansible_host=10.116.127.202 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="ui" ui=true extra_disks=false
#cloudforms        ansible_host=10.116.127.17 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="ui" ui=true extra_disks=false

#zone 919000000000003
[cloudforms-wk-ocp]
cloudforms-wk-ocp01   ansible_host=10.116.127.203 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="wk-ocp" wk-ocp=true extra_disks=false

#zone 919000000000004
[cloudforms-wk-osp]
cloudforms-wk-osp01   ansible_host=10.116.127.204 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="wk-osp" wk-osp=true extra_disks=false

#zone 919000000000005
[cloudforms-wk-rhev]
cloudforms-wk-rhev01   ansible_host=10.116.127.205 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" type="wk-rhev" wk-rhev=true extra_disks=false

#[test-rhel6]
#test-rhel6      ansible_host=10.116.127.18 activation_key="AK-CV-OS-RHEL6-SERVER-{{ stage }}"

[bosch-vpn-group]
bosch-vpn       l1host="layer1" ansible_host=10.116.127.19 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

#[ose3-master]
#ose3-master1    ansible_host=10.116.127.20 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"
#ose3-master2    ansible_host=10.116.127.21 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"
#ose3-master3    ansible_host=10.116.127.22 openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}"

#[ose3-node]
#ose3-infranode1 ansible_host=10.116.127.23 openshift_node_labels="{'region': 'infra', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}" additional_dns="*.apps,apps"
#ose3-node1      ansible_host=10.116.127.25 openshift_node_labels="{'region': 'primary', 'zone': 'east', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}"
#ose3-node2      ansible_host=10.116.127.26 openshift_node_labels="{'region': 'primary', 'zone': 'west', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}"

#[ose3-lb]
#ose3-lb         ansible_host=10.116.127.27 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="openshift"

[ose3-osp-master]
ose3-osp-master1  jump_host=ose3-osp-lb openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-master2  jump_host=ose3-osp-lb openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-master3  jump_host=ose3-osp-lb openshift_node_labels="{'region': 'master', 'zone': 'default', 'logging-infra-fluentd': 'true', 'openshift_schedulable': 'false'}" activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"

[ose3-osp-node]
ose3-osp-infranode1 openshift_node_labels="{'region': 'infra', 'zone': 'default', 'logging-infra-fluentd': 'true'}" activation_key="AK-CV-RHOSE3-{{ stage }}" additional_dns="*.apps-osp,apps-osp" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-node1      jump_host=ose3-osp-lb openshift_node_labels="{'region': 'primary', 'zone': 'east', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"
ose3-osp-node2      jump_host=ose3-osp-lb openshift_node_labels="{'region': 'primary', 'zone': 'west', 'logging-infra-fluentd': 'true'}"  activation_key="AK-CV-RHOSE3-{{ stage }}" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.large" osp_image="RHEL7"

[ose3-osp-lb]
ose3-osp-lb         activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="openshift-osp" osp_tenant="{{ ocp_tenant }}" osp_flavor="m1.medium" osp_image="RHEL7"

# Elasticsearch, FluentD, Kibana
[efk]
efk             ansible_host=10.116.127.176 activation_key="AK-CV-RHOSP-{{ stage }}"

# the Infrastructure server (DNS, SMTP/IMAP)
[infrastructure-group]
infrastructure  ansible_host=10.116.127.155 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" additional_dns="imap,smtp,ntp" roles=["dnat","smtp","haproxy","ntp"]
syslog		      ansible_host=10.116.127.9 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" roles=["syslog"]

# The single system where osp-director will be installed to
[rhosp-director]
rhosp-director  ansible_host=10.116.127.172 activation_key="AK-CV-RHOSP-{{ stage }}"

# The group of all osp compute nodes
[rhosp-compute]
rhosp-compute1  ansible_host=10.116.127.143 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute2  ansible_host=10.116.127.144 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute3  ansible_host=10.116.127.145 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-compute4  ansible_host=10.116.127.146 activation_key="AK-CV-RHOSP-{{ stage }}"

# The group of all osp control nodes
[rhosp-control]
rhosp-control1  ansible_host=10.116.127.140 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-control2  ansible_host=10.116.127.141 activation_key="AK-CV-RHOSP-{{ stage }}"
rhosp-control3  ansible_host=10.116.127.142 activation_key="AK-CV-RHOSP-{{ stage }}"

[dev-client]
#dev-client     ansible_host=10.116.127.37 activation_key="AK-CV-RHOSE3-CLIENT-{{ stage }}"

[tower]
#tower          ansible_host=10.116.127.38 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[lookbusy-rhev]
#lookbusy-rhev   ansible_host=10.116.127.39 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" rhev_image="RHEL7" nic_attachments="[ {{ infrastructure_network_services }},{{ infrastructure_network_admin }} ]"

[ceph]
#will be filled in dynamically via layer2_ceph_inventory role
#ceph1 ansible_host=10.116.127.2 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph2 ansible_host=10.116.127.3 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph3 ansible_host=10.116.127.4 activation_key="AK-CV-CEPH-{{ stage }}"
#ceph4 ansible_host=10.116.127.5 activation_key="AK-CV-CEPH-{{ stage }}"

[storage-console]
#storage-console ansible_host=10.116.127.9 activation_key="AK-CV-CEPH-{{ stage }}"

[install-host]
install-host ansible_host=10.116.127.154 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[rh-sso]
rh-sso ansible_host=10.116.127.7 activation_key="AK-CV-RH-SSO-{{ stage }}"

[proxy-group]
proxy ansible_host=10.116.127.8 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}"

[nuage]
vsd1 ansible_host=10.116.127.156 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}" srv_dns="['_xmpp-client._tcp.xmpp','1 100 5222']" additional_dns="xmpp"
vsd2 ansible_host=10.116.127.157 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}" srv_dns="['_xmpp-client._tcp.xmpp','2 100 5222']" additional_dns="xmpp"
vsd3 ansible_host=10.116.127.158 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}" srv_dns="['_xmpp-client._tcp.xmpp','3 100 5222']" additional_dns="xmpp"
c1 ansible_host=10.116.127.162 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsc1"
c2 ansible_host=10.116.127.163 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsc2"
c3 ansible_host=10.116.127.164 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsc3"
c4 ansible_host=10.116.127.165 nic_attachments="[ {{ infrastructure_network_services }}, {{ infrastructure_network_tenant }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsc4"
elastic1 ansible_host=10.116.127.159 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
elastic2 ansible_host=10.116.127.160 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
elastic3 ansible_host=10.116.127.161 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
r1 ansible_host=10.116.127.166 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsr1"
r2 ansible_host=10.116.127.168 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}" additional_dns="vsr2"

# virtual IPs - these will not be instantiated through Ansible (but need them for DNS records, DNAT, etc)
[vips]
openstack         ansible_host=10.116.127.175 nic_attachments="[ {{ infrastructure_network_services }} ]" default_route_via="{{ infrastructure_network_services }}"
undercloud-public ansible_host=10.116.127.173 nic_attachments="[ {{ infrastructure_network_provisioning }} ]" default_route_via={{infrastructure_network_provisioning}} additional_dns="director"
undercloud-admin  ansible_host=10.116.127.174 nic_attachments="[ {{ infrastructure_network_provisioning }} ]"

# layer 3 openstack
[lookbusy-osp]
lookbusy-osp    activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"

# layer 3 openstack
[lbaas-demo-osp-group]
lbaas-demo-osp1 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"
lbaas-demo-osp2 activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"


[cloudforms-osp-group]
cloudforms-osp ansible_host=10.116.127.121 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.xlarge" osp_image="Cloudforms"
cloudforms-osp-2 ansible_host=10.116.127.122 activation_key="AK-CV-CLOUDFORMS-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.xlarge" osp_image="Cloudforms"

[ansible-jump-host]
ansible-jump-demo-vms  activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-vms" osp_flavor="m1.small" osp_image="RHEL7"
#ansible-jump-demo-infra  activation_key="AK-CV-OS-RHEL7-SERVER-{{ stage }}" osp_tenant="demo-infra" osp_flavor="m1.small" osp_image="RHEL7"

[laptop]
localhost              ansible_connection=local openshift_master_api_host_and_port='openshift.{{ hailstorm_dns_domain }}:443'


#### ADDITIONAL GROUPS

# which system will host virt-who -> this should not be satellite since virt-who reports to subscription-manager which reports to RHN (which doesn't know the Default Organization) on satellite
[virt-who]
infrastructure

# the list of groups that make up RHEV
[rhev:children]
rhevm
rhevh

# all RHEV nodes and the layer1 host to ensure there's a common view on the storage domain
[layer1-rhev:children]
rhev
layer1

# A list of all OpenStack subgroups
[rhosp-all:children]
rhosp-director
rhosp-undercloud

# The groups that make up the osp undercloud (i.e. compute, control and potentially storage)
[rhosp-undercloud:children]
rhosp-compute
rhosp-control

# a group required to store a common configuration for IPMI emulation between the layer1 host and the osp-director
[layer1-rhosp]
rhosp-director
layer1

# all openshift nodes on bare metal
#[ose3:children]
#ose3-master
#ose3-node
#ose3-lb

# all openshift nodes on OSP
[ose3-osp:children]
ose3-osp-master
ose3-osp-node
ose3-osp-lb

# common config items for openshift masters and nodes on bare metal
#[ose3-common:children]
#ose3-master
#ose3-node

# common config items for openshift masters and nodes on OSP
[ose3-osp-common:children]
ose3-osp-master
ose3-osp-node

# the node where the ansible playbook will run
#[ose3-installer]
#ose3-master1

# the node where the ansible playbook will run on OSP
[ose3-osp-installer]
ose3-osp-master1

# All layer2 node groups that are installed with RHEL7
[rhel7:children]
proxy-group
install-host
rh-sso
rhosp-all
satellite
rhevh
#ose3
ose3-osp
ipa
infrastructure-group
efk
lookbusy
dev-client
tower
#cloudforms
cloudforms-db
cloudforms-ui
cloudforms-wk-ocp
cloudforms-wk-osp
cloudforms-wk-rhev
cloudforms-osp-group
ansible-jump-host
storage-console
bosch-vpn-group

# All layer2 node groups that are installed with RHEL6
[rhel6:children]
#rhevm #group assignment (rhel6 or rhel7) now done via role
#test-rhel6

# everything on layer2
[layer2:children]
proxy-group
install-host
rh-sso
nuage
#ose3
satellite
rhev
rhosp-all
ipa
infrastructure-group
efk
dev-client
tower
storage-console
bosch-vpn-group

# to let it participate in the calculation of ansible_host
# and the ssh proxy command used to communicate with all layer2/3 hosts
[accessible_via_underlay:children]
layer2
#cloudforms
cloudforms-db
cloudforms-ui
cloudforms-wk-ocp
cloudforms-wk-osp
cloudforms-wk-rhev
cloudforms-osp-group
vips
lookbusy-rhev
bosch-vpn-group

[accessible_via_floating_ip:children]
ansible-jump-host
lookbusy-osp
lbaas-demo-osp-group
ose3-osp

[accessible_via_services_network:children]
nuage

# to let it participate in the calculation of vm_nics
[automated_vmnics_calculation:children]
accessible_via_floating_ip
accessible_via_underlay
accessible_via_services_network

# A group to capture all nodes with standard 3-NIC network layout (services, admin, storage)
[niclayout-standard:children]
install-host
rh-sso
nuage
satellite
infrastructure-group
ipa
#ose3
efk
rhevm
lookbusy-rhev
dev-client
tower
storage-console
bosch-vpn-group

[lookbusy:children]
lookbusy-osp
lbaas-demo-osp-group
lookbusy-rhev

[haproxy_frontend]
infrastructure

[haproxy_backend]
ose3-master1
ose3-master2
ose3-master3
